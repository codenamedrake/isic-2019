{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Lesion Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to mount Google Drive for Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "# !ls '/content/drive/My Drive/Colab Notebooks'\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks/isic-2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to mount your Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp '/content/drive/My Drive/Colab Notebooks/ISIC_2019_Training_Input.zip' '/home/ISIC_2019_Training_Input.zip'\n",
    "# !cp '/content/drive/My Drive/Colab Notebooks/ISIC_2019_Training_GroundTruth.csv' '/home/ISIC_2019_Training_GroundTruth.csv'\n",
    "# !unzip -qq '/home/ISIC_2019_Training_Input.zip' -d '/home'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether youâ€™re running Pillow or Pillow-SIMD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the author, if PILLOW_VERSION has a postfix, it is Pillow-SIMD0.\n",
    "# (Assuming that Pillow will never make a .postX release).\n",
    "!python -c \"from PIL import Image; print(Image.PILLOW_VERSION)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whether Pillow or Pillow-SIMD is using libjpeg-turbo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import features, Image\n",
    "from packaging import version\n",
    "\n",
    "if version.parse(Image.PILLOW_VERSION) >= version.parse(\"5.4.0\"):\n",
    "    if features.check_feature('libjpeg_turbo'):\n",
    "        print(\"libjpeg-turbo is on\")\n",
    "    else:\n",
    "        print(\"libjpeg-turbo is not on\")\n",
    "else:\n",
    "    print(\"libjpeg-turbo' status can't be derived - need Pillow(-SIMD)? >= 5.4.0 to tell, current version {}\".format(Image.PILLOW_VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import platform\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "!python3 --version\n",
    "\n",
    "print('\\nTensorFlow Version: ', tf.VERSION)\n",
    "\n",
    "print('\\nNVIDIA:')\n",
    "!nvcc --version\n",
    "# !nvidia-smi\n",
    "\n",
    "print('\\nCPU:')\n",
    "!lscpu\n",
    "\n",
    "print('\\nOS:')\n",
    "print(platform.platform())\n",
    "\n",
    "print('\\nDevices:')\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# dermoscopic images folder path\n",
    "data_folder = 'C:\\ISIC_2019'\n",
    "# data_folder = '/home'\n",
    "# data_folder = '/home/jupyter'\n",
    "derm_image_folder = os.path.join(data_folder, 'ISIC_2019_Training_Input')\n",
    "df_ground_truth = pd.read_csv(os.path.join(data_folder, 'ISIC_2019_Training_GroundTruth.csv'))\n",
    "\n",
    "# Category names not include UNK\n",
    "category_names = list(df_ground_truth.columns.values[1:9])\n",
    "known_category_num = len(category_names)\n",
    "print(\"Number of known categories: {}\".format(known_category_num))\n",
    "print(category_names, '\\n')\n",
    "\n",
    "# mapping from category to index\n",
    "print('Category to Index:')\n",
    "category_to_index = dict((c, i) for i, c in enumerate(category_names))\n",
    "print(category_to_index, '\\n')\n",
    "\n",
    "df_ground_truth['path'] = df_ground_truth.apply(lambda row : os.path.join(derm_image_folder, row['image']+'.jpg'), axis=1)\n",
    "df_ground_truth['category'] = pd.Series([np.argmax(x) for x in np.array(df_ground_truth.iloc[:,1:9])], name='category')\n",
    "count_per_category = Counter(df_ground_truth['category'])\n",
    "total_sample_count = sum(count_per_category.values())\n",
    "print(\"Original training data has {} samples.\".format(total_sample_count))\n",
    "\n",
    "for i, c in enumerate(category_names):\n",
    "    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_per_category[i], count_per_category[i]*100/total_sample_count))\n",
    "\n",
    "fig = plt.bar(count_per_category.keys(), count_per_category.values())\n",
    "\n",
    "df_ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and Split Original Training Data into Training  and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 1\n",
    "df_train, df_val = train_test_split(df_ground_truth, stratify=df_ground_truth['category'], test_size=0.2, random_state=seed)\n",
    "\n",
    "sample_count_train = df_train.shape[0]\n",
    "print(\"Training set has {} samples.\".format(sample_count_train))\n",
    "count_per_category_train = Counter(df_train['category'])\n",
    "for i, c in enumerate(category_names):\n",
    "    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_per_category_train[i], count_per_category_train[i]*100/sample_count_train))\n",
    "    \n",
    "sample_count_val = df_val.shape[0]\n",
    "print(\"\\nValidation set has {} samples.\".format(sample_count_val))\n",
    "count_per_category_val = Counter(df_val['category'])\n",
    "for i, c in enumerate(category_names):\n",
    "    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_per_category_val[i], count_per_category_val[i]*100/sample_count_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weights based on the Traning Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Compute class weights for imbalanced data\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(df_train['category']), df_train['category'])\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "# class_weight_dict = dict(zip(category_names, class_weights))\n",
    "print('Class Weights:')\n",
    "print(class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples of each Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "category_groups = df_train.groupby('category')\n",
    "\n",
    "# Number of samples for each category\n",
    "num_per_category = 3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=known_category_num, ncols=num_per_category, figsize=(9, 24))\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "for idx, val in enumerate(category_names):\n",
    "    i = 0\n",
    "    for index, row in category_groups.get_group(idx).head(num_per_category).iterrows():\n",
    "        ax = axes[idx, i]\n",
    "        ax.imshow(plt.imread(row['path']))\n",
    "        ax.set_xlabel(row['image'])\n",
    "        if ax.is_first_col():\n",
    "            ax.set_ylabel(val, fontsize=20)\n",
    "            ax.yaxis.label.set_color('blue')\n",
    "        i += 1\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path, size=(224, 224)):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=size)\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths, size=(224, 224)):\n",
    "    list_of_tensors = [path_to_tensor(img_path, size) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://stackoverflow.com/a/54620037/2437361\n",
    "import keras.backend as K\n",
    "\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the average per-class recall metric for a multi-class classification problem\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)  \n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0)   \n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    balanced_recall = K.mean(recall)\n",
    "    return balanced_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vanilla CNN as benchmark model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_iterator import ImageIterator\n",
    "from Augmentor import Pipeline\n",
    "from Augmentor.Operations import CropPercentageRange\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "\n",
    "input_size = (224, 224)\n",
    "batch_size = 40\n",
    "data_format = K.image_data_format()\n",
    "\n",
    "### Training Data Generator\n",
    "#TODO Maybe remove black borders\n",
    "vanilla_p_train = Pipeline()\n",
    "# Random crop\n",
    "vanilla_p_train.add_operation(CropPercentageRange(probability=1, min_percentage_area=0.8, max_percentage_area=1, centre=False))\n",
    "# Rotate an image by either 90, 180, or 270 degrees randomly\n",
    "vanilla_p_train.rotate_random_90(probability=0.5)\n",
    "# Resize an image\n",
    "vanilla_p_train.resize(probability=1, width=input_size[0], height=input_size[1])\n",
    "# Flip the image along its vertical axis\n",
    "vanilla_p_train.flip_top_bottom(probability=0.5)\n",
    "# Flip the image along its horizontal axis\n",
    "vanilla_p_train.flip_left_right(probability=0.5)\n",
    "# Random change brightness of an image\n",
    "vanilla_p_train.random_brightness(probability=0.5, min_factor=0.9, max_factor=1.1)\n",
    "# Random change saturation of an image\n",
    "vanilla_p_train.random_color(probability=0.5, min_factor=0.9, max_factor=1.1)\n",
    "# Set the seed\n",
    "vanilla_p_train.set_seed(seed)\n",
    "vanilla_p_train.status()\n",
    "\n",
    "generator_train = ImageIterator(\n",
    "    image_paths=df_train['path'].tolist(),\n",
    "    labels=np_utils.to_categorical(df_train['category'], num_classes=known_category_num),\n",
    "    augmentation_pipeline=vanilla_p_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    rescale=1./255,\n",
    "    data_format=data_format\n",
    ")\n",
    "\n",
    "\n",
    "### Validation Data Generator\n",
    "vanilla_p_val = Pipeline()\n",
    "# Resize an image\n",
    "vanilla_p_val.resize(probability=1, width=input_size[0], height=input_size[1])\n",
    "# Set the seed\n",
    "vanilla_p_val.set_seed(seed)\n",
    "vanilla_p_val.status()\n",
    "\n",
    "generator_val = ImageIterator(\n",
    "    image_paths=df_val['path'].tolist(),\n",
    "    labels=np_utils.to_categorical(df_val['category'], num_classes=known_category_num),\n",
    "    augmentation_pipeline=vanilla_p_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    rescale=1./255,\n",
    "    data_format=data_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras import backend as K\n",
    "# from Augmentor import DataFramePipeline\n",
    "# from Augmentor.Operations import CropPercentageRange\n",
    "\n",
    "# input_size = (224, 224)\n",
    "# batch_size = 40\n",
    "\n",
    "# ### Training Data Generator\n",
    "# #TODO Maybe remove black borders\n",
    "# vanilla_p_train = DataFramePipeline(source_dataframe=df_train, image_col='path', category_col='category')\n",
    "# # Random crop\n",
    "# vanilla_p_train.add_operation(CropPercentageRange(probability=1, min_percentage_area=0.8, max_percentage_area=1, centre=False))\n",
    "# # Rotate an image by either 90, 180, or 270 degrees randomly\n",
    "# vanilla_p_train.rotate_random_90(probability=0.5)\n",
    "# # Resize an image\n",
    "# vanilla_p_train.resize(probability=1, width=input_size[0], height=input_size[1])\n",
    "# # Flip the image along its vertical axis\n",
    "# vanilla_p_train.flip_top_bottom(probability=0.5)\n",
    "# # Flip the image along its horizontal axis\n",
    "# vanilla_p_train.flip_left_right(probability=0.5)\n",
    "# # Random change brightness of an image\n",
    "# vanilla_p_train.random_brightness(probability=1, min_factor=0.9, max_factor=1.1)\n",
    "# # Random change saturation of an image\n",
    "# vanilla_p_train.random_color(probability=1, min_factor=0.9, max_factor=1.1)\n",
    "# # Set the seed\n",
    "# vanilla_p_train.set_seed(seed)\n",
    "\n",
    "# generator_train = vanilla_p_train.keras_generator(batch_size=batch_size, scaled=True, image_data_format=K.image_data_format())\n",
    "# # vanilla_p_train.status()\n",
    "\n",
    "# ### Validation Data Generator\n",
    "# vanilla_p_val = DataFramePipeline(source_dataframe=df_val, image_col='path', category_col='category')\n",
    "# # Center crop\n",
    "# vanilla_p_val.crop_centre(probability=1, percentage_area=0.9)\n",
    "# # Resize an image\n",
    "# vanilla_p_val.resize(probability=1, width=input_size[0], height=input_size[1])\n",
    "# # Set the seed\n",
    "# vanilla_p_val.set_seed(seed)\n",
    "\n",
    "# generator_val = vanilla_p_val.keras_generator(batch_size=batch_size, scaled=True, image_data_format=K.image_data_format())\n",
    "# # vanilla_p_val.status()\n",
    "\n",
    "# # datagen_train = ImageDataGenerator(rescale=1./255)\n",
    "# # generator_train = datagen_train.flow_from_dataframe(\n",
    "# #     dataframe=df_train, x_col='path', y_col='category', class_mode='categorical', target_size=input_size, batch_size=batch_size, seed=seed)\n",
    "\n",
    "# # datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "# # generator_val = datagen_val.flow_from_dataframe(\n",
    "# #     dataframe=df_val, x_col='path', y_col='category', class_mode='categorical', target_size=input_size, batch_size=batch_size, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print some info for debugging\n",
    "# images, labels = next(generator_train)\n",
    "# # print(len(vanilla_p_train.augmentor_images))\n",
    "# print(images.shape)\n",
    "# print(labels.shape)\n",
    "# plt.imshow(images[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "lr_start = 1e-3 # Starting learning rate\n",
    "\n",
    "# Define vanilla CNN\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(input_size[0], input_size[1], 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(known_category_num, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=lr_start), loss='categorical_crossentropy', metrics=[balanced_accuracy, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dispaly images of a batch for debugging\n",
    "# images, labels = next(generator_train)\n",
    "\n",
    "# fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "# for i, img in enumerate(images):\n",
    "#     ax = fig.add_subplot(4, 10, i + 1, xticks=[], yticks=[])\n",
    "#     ax.imshow(np.uint8(255 * img))\n",
    "#     image_idx = np.argmax(labels[i])\n",
    "#     ax.set(title=category_names[image_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the vanilla CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "checkpoint_balanced_acc = ModelCheckpoint(\n",
    "    filepath='saved_models/vanilla_best_balanced_acc.hdf5',\n",
    "    monitor='val_balanced_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "checkpoint_loss = ModelCheckpoint(\n",
    "    filepath='saved_models/vanilla_best_loss.hdf5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "# Reduce learning rate when the validation loss has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Stop training when the validation loss has stopped improving.\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=22, verbose=1)\n",
    "\n",
    "# Callback that streams epoch results to a csv file.\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "csv_logger = CSVLogger('logs/vanilla.training.csv', append=True)\n",
    "\n",
    "epoch_num = 100\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generator_train,\n",
    "    class_weight=class_weight_dict,\n",
    "    max_queue_size=10,\n",
    "    workers=2,\n",
    "    use_multiprocessing=False,\n",
    "    steps_per_epoch=sample_count_train//batch_size,\n",
    "    epochs=epoch_num,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint_balanced_acc, checkpoint_loss,\n",
    "               reduce_lr, early_stop, csv_logger],\n",
    "    validation_data=generator_val,\n",
    "    validation_steps=sample_count_val//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visuals import *\n",
    "\n",
    "plot_complexity_graph('logs/vanilla.training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "vanilla_model = load_model(\n",
    "    filepath='saved_models/vanilla_best_balanced_acc.hdf5',\n",
    "    custom_objects={'balanced_accuracy': balanced_accuracy})\n",
    "# vanilla_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Dermoscopic Images with the Vanilla CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def vanilla_classify(img_path, topk=5):\n",
    "    predicted_vector = vanilla_model.predict(path_to_tensor(img_path))\n",
    "    idx_topk = np.argsort(-predicted_vector)[0, :topk]\n",
    "    probs = np.take(predicted_vector, idx_topk)\n",
    "    names = [category_names[idx] for idx in idx_topk]\n",
    "    \n",
    "    return idx_topk, names, probs\n",
    "\n",
    "topk = 8\n",
    "df_row = df_val.iloc[random.randrange(len(df_val.index))]\n",
    "idx_topk, names, probs = vanilla_classify(df_row['path'], topk=topk)\n",
    "# print(probs)\n",
    "\n",
    "# Set up plot\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# Set up title\n",
    "fig.suptitle(df_row['image'])\n",
    "\n",
    "# Input Image\n",
    "ax1.set_title(category_names[df_row['category']])\n",
    "ax1.imshow(plt.imread(df_row['path']))\n",
    "\n",
    "# Plot probabilities bar chart\n",
    "ax2.set_title(\"Top {0} probabilities\".format(topk))\n",
    "ax2.barh(np.arange(topk), probs)\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(topk))\n",
    "ax2.set_yticklabels(names, size='medium')\n",
    "ax2.yaxis.tick_right()\n",
    "ax2.set_xlim(0, 1.0)\n",
    "ax2.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation Pipeline for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transfer_learning import build_aug_pipeline\n",
    "\n",
    "p_train, p_val = build_aug_pipeline(\n",
    "#     train_dataframe=df_train, val_dataframe=df_val,\n",
    "#     image_col='path', category_col='category',\n",
    "    input_size=(224, 224), seed=seed\n",
    ")\n",
    "\n",
    "print('Training Augmentation:')\n",
    "p_train.status()\n",
    "print('Validation Augmentation:')\n",
    "p_val.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transfer_learning import build_finetune_model\n",
    "from keras.applications.densenet import DenseNet201, preprocess_input as preprocess_input_densenet\n",
    "\n",
    "batch_size = 40\n",
    "lr_start = 1e-3 # Starting learning rate\n",
    "preprocessing_func = preprocess_input_densenet\n",
    "data_format = K.image_data_format()\n",
    "\n",
    "generator_train = ImageIterator(\n",
    "    image_paths=df_train['path'].tolist(),\n",
    "    labels=np_utils.to_categorical(df_train['category'], num_classes=known_category_num),\n",
    "    augmentation_pipeline=p_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    preprocessing_function=preprocessing_func,\n",
    "    data_format=data_format\n",
    ")\n",
    "\n",
    "generator_val = ImageIterator(\n",
    "    image_paths=df_val['path'].tolist(),\n",
    "    labels=np_utils.to_categorical(df_val['category'], num_classes=known_category_num),\n",
    "    augmentation_pipeline=p_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    preprocessing_function=preprocessing_func,\n",
    "    data_format=data_format\n",
    ")\n",
    "\n",
    "base_model = DenseNet201(include_top=False, weights='imagenet')\n",
    "base_model_name = 'DenseNet201'\n",
    "fc_layers = [512]\n",
    "dropout = 0.3\n",
    "\n",
    "densenet_model = build_finetune_model(\n",
    "    base_model,\n",
    "    fc_layers=fc_layers,\n",
    "    dropout=dropout,\n",
    "    num_classes=known_category_num,\n",
    "    base_model_layers_trainable=False)\n",
    "\n",
    "densenet_model.summary()\n",
    "\n",
    "# Compile the model\n",
    "densenet_model.compile(optimizer=Adam(lr=lr_start), loss='categorical_crossentropy', metrics=[balanced_accuracy, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print some info for debugging\n",
    "# images, labels = next(generator_train)\n",
    "# print(images.shape)\n",
    "# # print(images[0])\n",
    "# # plt.imshow(images[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "    \n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "checkpoint_balanced_acc = ModelCheckpoint(\n",
    "    filepath='saved_models/{}_best_balanced_acc.hdf5'.format(base_model_name),\n",
    "    monitor='val_balanced_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "checkpoint_loss = ModelCheckpoint(\n",
    "    filepath='saved_models/{}_best_loss.hdf5'.format(base_model_name),\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "# Reduce learning rate when the validation loss has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Stop training when the validation loss has stopped improving.\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=22, verbose=1)\n",
    "\n",
    "# Callback that streams epoch results to a csv file.\n",
    "csv_logger = CSVLogger('logs/{}.training.csv'.format(base_model_name), append=True)\n",
    "\n",
    "epoch_num = 125\n",
    "\n",
    "history = densenet_model.fit_generator(\n",
    "    generator_train,\n",
    "    class_weight=class_weight_dict,\n",
    "    max_queue_size=10,\n",
    "    workers=2,\n",
    "    use_multiprocessing=False,\n",
    "    steps_per_epoch=sample_count_train//batch_size,\n",
    "    epochs=epoch_num,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint_balanced_acc, checkpoint_loss,\n",
    "               reduce_lr, early_stop, csv_logger],\n",
    "    validation_data=generator_val,\n",
    "    validation_steps=sample_count_val//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visuals import *\n",
    "\n",
    "plot_complexity_graph('logs/{}.training.csv'.format(base_model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
