{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Lesion Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to mount Google Drive for Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "# !ls '/content/drive/My Drive/Colab Notebooks'\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks/isic-2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp '/content/drive/My Drive/Colab Notebooks/ISIC_2019_Training_Input.zip' '/home/ISIC_2019_Training_Input.zip'\n",
    "# !cp '/content/drive/My Drive/Colab Notebooks/ISIC_2019_Training_GroundTruth.csv' '/home/ISIC_2019_Training_GroundTruth.csv'\n",
    "# !cp '/content/drive/My Drive/Colab Notebooks/ISIC_Archive_Out_Distribtion.zip' '/home/ISIC_Archive_Out_Distribtion.zip'\n",
    "# !cp '/content/drive/My Drive/Colab Notebooks/Iris_Out_Distribtion.zip' '/home/Iris_Out_Distribtion.zip'\n",
    "# !unzip -qq '/home/ISIC_2019_Training_Input.zip' -d '/home'\n",
    "# !unzip -qq '/home/ISIC_Archive_Out_Distribtion.zip' -d '/home'\n",
    "# !unzip -qq '/home/Iris_Out_Distribtion.zip' -d '/home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ref https://docs.fast.ai/performance.html\n",
    "# !pip uninstall -y pillow pil jpeg libtiff libjpeg-turbo\n",
    "# !CFLAGS=\"${CFLAGS} -mavx2\" pip install --upgrade --no-cache-dir --force-reinstall --no-binary :all: --compile pillow-simd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether youâ€™re running Pillow or Pillow-SIMD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the author, if PILLOW_VERSION has a postfix, it is Pillow-SIMD0.\n",
    "# (Assuming that Pillow will never make a .postX release).\n",
    "!python3 -c \"from PIL import Image; print(Image.PILLOW_VERSION)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whether Pillow or Pillow-SIMD is using libjpeg-turbo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import features, Image\n",
    "from packaging import version\n",
    "\n",
    "if version.parse(Image.PILLOW_VERSION) >= version.parse(\"5.4.0\"):\n",
    "    if features.check_feature('libjpeg_turbo'):\n",
    "        print(\"libjpeg-turbo is on\")\n",
    "    else:\n",
    "        print(\"libjpeg-turbo is not on\")\n",
    "else:\n",
    "    print(\"libjpeg-turbo' status can't be derived - need Pillow(-SIMD)? >= 5.4.0 to tell, current version {}\".format(Image.PILLOW_VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm TensorFlow can see the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print(\"Found GPU at: {}\".format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import platform\n",
    "from tensorflow.python.client import device_lib\n",
    "import keras\n",
    "\n",
    "!python3 --version\n",
    "\n",
    "print('\\nKeras Version: ', keras.__version__)\n",
    "print('\\nTensorFlow Version: ', tf.VERSION)\n",
    "\n",
    "print('\\nNVIDIA:')\n",
    "!nvcc --version\n",
    "!nvidia-smi\n",
    "!nvidia-smi topo -m\n",
    "\n",
    "print('\\nCPU:')\n",
    "!lscpu\n",
    "\n",
    "print('\\nMemory:')\n",
    "!cat /proc/meminfo\n",
    "\n",
    "print('\\nOS:')\n",
    "print(platform.platform())\n",
    "\n",
    "print('\\nDevices:')\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "data_folder = 'C:\\ISIC_2019'\n",
    "# data_folder = '/home'\n",
    "# data_folder = '/home/jupyter'\n",
    "# data_folder = '/home/ubuntu'\n",
    "\n",
    "saved_model_folder = 'saved_models'\n",
    "log_folder = 'logs'\n",
    "pred_result_folder = 'predict_results'\n",
    "out_dist_pred_result_folder = 'out_dist_predict_results'\n",
    "    \n",
    "workers = os.cpu_count()\n",
    "\n",
    "# How to handle SVG fonts\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "batch_size = 32\n",
    "epoch_num = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from data import load_isic_data\n",
    "from visuals import autolabel\n",
    "\n",
    "# dermoscopic images folder path\n",
    "derm_image_folder = os.path.join(data_folder, 'ISIC_2019_Training_Input')\n",
    "ground_truth_file = os.path.join(data_folder, 'ISIC_2019_Training_GroundTruth.csv')\n",
    "\n",
    "df_ground_truth, category_names = load_isic_data(derm_image_folder, ground_truth_file)\n",
    "known_category_num = len(category_names)\n",
    "print(\"Number of known categories: {}\".format(known_category_num))\n",
    "print(category_names, '\\n')\n",
    "\n",
    "# mapping from category to index\n",
    "print('Category to Index:')\n",
    "category_to_index = dict((c, i) for i, c in enumerate(category_names))\n",
    "print(category_to_index, '\\n')\n",
    "\n",
    "count_per_category = Counter(df_ground_truth['category'])\n",
    "total_sample_count = sum(count_per_category.values())\n",
    "print(\"Original training data has {} samples.\".format(total_sample_count))\n",
    "\n",
    "for i, c in enumerate(category_names):\n",
    "    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_per_category[i], count_per_category[i]*100/total_sample_count))\n",
    "\n",
    "# Create a bar chart\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "fig.patch.set_facecolor('white')\n",
    "# plt.bar(count_per_category.keys(), count_per_category.values())\n",
    "rects = plt.bar(category_names, [count_per_category[i] for i in range(known_category_num)])\n",
    "autolabel(ax, rects)\n",
    "fig.tight_layout()\n",
    "\n",
    "df_ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and Split Original Training Data into Training  and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import train_validation_split\n",
    "from visuals import plot_grouped_2bars\n",
    "\n",
    "df_train, df_val = train_validation_split(df_ground_truth)\n",
    "\n",
    "# Training Set\n",
    "sample_count_train = df_train.shape[0]\n",
    "print(\"Training set has {} samples.\".format(sample_count_train))\n",
    "count_per_category_train = Counter(df_train['category'])\n",
    "for i, c in enumerate(category_names):\n",
    "    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_per_category_train[i], count_per_category_train[i]*100/sample_count_train))\n",
    "\n",
    "# Validation Set\n",
    "sample_count_val = df_val.shape[0]\n",
    "print(\"\\nValidation set has {} samples.\".format(sample_count_val))\n",
    "count_per_category_val = Counter(df_val['category'])\n",
    "for i, c in enumerate(category_names):\n",
    "    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_per_category_val[i], count_per_category_val[i]*100/sample_count_val))\n",
    "\n",
    "plot_grouped_2bars(\n",
    "    scalars=[[count_per_category_train[i] for i in range(known_category_num)],\n",
    "             [count_per_category_val[i] for i in range(known_category_num)]],\n",
    "    scalarlabels=['Training', 'Validation'],\n",
    "    xticklabels=category_names,\n",
    "    title='Distribution of Training and Validation Sets'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weights based on the Traning Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import compute_class_weight_dict\n",
    "\n",
    "class_weight_dict, class_weights = compute_class_weight_dict(df_train)\n",
    "print('Class Weights Dictionary:')\n",
    "print(class_weight_dict)\n",
    "\n",
    "# Create a bar chart\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.set_title('Class Weights')\n",
    "plt.bar(category_names, [class_weight_dict[i] for i in range(known_category_num)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-channel Mean and Standard Deviation over the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calculate_mean_std\n",
    "\n",
    "### Uncomment below codes to calculate per-channel mean and standard deviation over the training set\n",
    "# rgb_mean, rgb_std = calculate_mean_std(df_train['path'])\n",
    "# print(\"Mean:{}\\nSTD:{}\".format(rgb_mean, rgb_std))\n",
    "\n",
    "# Output was:\n",
    "# Mean:[0.6236094091893962, 0.5198354883713194, 0.5038435406338101]\n",
    "# STD:[0.2421814437693499, 0.22354427793687906, 0.2314805420919389]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples of each Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "category_groups = df_train.groupby('category')\n",
    "\n",
    "# Number of samples for each category\n",
    "num_per_category = 3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=known_category_num, ncols=num_per_category, figsize=(9, 24))\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "for idx, val in enumerate(category_names):\n",
    "    i = 0\n",
    "    for index, row in category_groups.get_group(idx).head(num_per_category).iterrows():\n",
    "        ax = axes[idx, i]\n",
    "        ax.imshow(plt.imread(row['path']))\n",
    "        ax.set_xlabel(row['image'])\n",
    "        if ax.is_first_col():\n",
    "            ax.set_ylabel(val, fontsize=20)\n",
    "            ax.yaxis.label.set_color('blue')\n",
    "        i += 1\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Vanilla CNN as Benchmark Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Vanilla CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import train_vanilla\n",
    "\n",
    "train_vanilla(\n",
    "    df_train=df_train,\n",
    "    df_val=df_val,\n",
    "    known_category_num=known_category_num,\n",
    "    class_weight_dict=class_weight_dict,\n",
    "    batch_size=batch_size,\n",
    "    max_queue_size=10,\n",
    "    epoch_num=epoch_num,\n",
    "    input_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity Graph of Vanilla CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visuals import *\n",
    "\n",
    "fig = plot_complexity_graph(csv_file=os.path.join(log_folder, 'Vanilla.training.csv'),\n",
    "                            title='Complexity Graph of Vanilla CNN')\n",
    "fig.savefig(os.path.join(log_folder, 'Vanilla.training.svg'), format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Dermoscopic Images with the Vanilla CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utils import path_to_tensor\n",
    "\n",
    "def vanilla_classify(img_path, topk=5):\n",
    "    # TODO: Use Image Augmentation Pipeline instead of path_to_tensor\n",
    "    # Note that path_to_tensor does not rescale images like VanillaClassifier.preprocess_input\n",
    "    predicted_vector = model.predict(path_to_tensor(img_path, size=(224, 224)))\n",
    "    idx_topk = np.argsort(-predicted_vector)[0, :topk]\n",
    "    probs = np.take(predicted_vector, idx_topk)\n",
    "    names = [category_names[idx] for idx in idx_topk]\n",
    "    \n",
    "    return idx_topk, names, probs\n",
    "\n",
    "topk = known_category_num\n",
    "df_row = df_val.iloc[random.randrange(len(df_val.index))]\n",
    "idx_topk, names, probs = vanilla_classify(df_row['path'], topk=topk)\n",
    "# print(probs)\n",
    "\n",
    "# Set up plot\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# Set up title\n",
    "fig.suptitle(df_row['image'])\n",
    "\n",
    "# Input Image\n",
    "ax1.set_title(category_names[df_row['category']])\n",
    "ax1.imshow(plt.imread(df_row['path']))\n",
    "\n",
    "# Plot probabilities bar chart\n",
    "ax2.set_title(\"Top {0} probabilities\".format(topk))\n",
    "ax2.barh(np.arange(topk), probs)\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(topk))\n",
    "ax2.set_yticklabels(names, size='medium')\n",
    "ax2.yaxis.tick_right()\n",
    "ax2.set_xlim(0, 1.0)\n",
    "ax2.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models by Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from main import train_transfer_learning\n",
    "from base_model_param import get_transfer_model_param_map\n",
    "\n",
    "transfer_models = ['DenseNet201', 'Xception', 'ResNeXt50']\n",
    "model_param_map = get_transfer_model_param_map()\n",
    "base_model_params = [model_param_map[x] for x in transfer_models]\n",
    "    \n",
    "train_transfer_learning(\n",
    "    base_model_params=base_model_params,\n",
    "    df_train=df_train,\n",
    "    df_val=df_val,\n",
    "    known_category_num=known_category_num,\n",
    "    class_weight_dict=class_weight_dict,\n",
    "    batch_size=batch_size,\n",
    "    max_queue_size=10,\n",
    "    epoch_num=epoch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity Graph of Transfer Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from visuals import *\n",
    "\n",
    "model_names = ['DenseNet201', 'Xception', 'ResNeXt50']\n",
    "feature_extract_epochs = 3\n",
    "\n",
    "for model_name in model_names:\n",
    "    file_path = os.path.join(log_folder, \"{}.training.csv\".format(model_name))\n",
    "    if os.path.exists(file_path):\n",
    "        fig = plot_complexity_graph(csv_file=file_path,\n",
    "                              title=\"Complexity Graph of {}\".format(model_name),\n",
    "                              figsize=(14, 10),\n",
    "                              feature_extract_epochs=feature_extract_epochs)\n",
    "        fig.savefig(os.path.join(log_folder, \"{}.training.svg\".format(model_name)), format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Different Models to Predict Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from metrics import balanced_accuracy\n",
    "from lesion_classifier import LesionClassifier\n",
    "from vanilla_classifier import VanillaClassifier\n",
    "from base_model_param import get_transfer_model_param_map\n",
    "from keras import backend as K\n",
    "\n",
    "if not os.path.exists(pred_result_folder):\n",
    "    os.makedirs(pred_result_folder)\n",
    "\n",
    "model_name = 'Vanilla'\n",
    "postfixes = ['best_balanced_acc', 'best_loss', 'latest']\n",
    "model_param_map = get_transfer_model_param_map()\n",
    "        \n",
    "for postfix in postfixes:\n",
    "    print('postfix: ', postfix)\n",
    "    # Load model\n",
    "    model = load_model(filepath=os.path.join(saved_model_folder, \"{}_{}.hdf5\".format(model_name, postfix)),\n",
    "                       custom_objects={'balanced_accuracy': balanced_accuracy(known_category_num)})\n",
    "    # model.summary()\n",
    "\n",
    "    # Predict validation set\n",
    "    df_pred = LesionClassifier.predict_dataframe(\n",
    "        model=model, df=df_val,\n",
    "        category_names=category_names,\n",
    "        augmentation_pipeline=LesionClassifier.create_aug_pipeline_val((224, 224)), # only for Vanilla\n",
    "        preprocessing_function=VanillaClassifier.preprocess_input, # only for Vanilla\n",
    "        # augmentation_pipeline=LesionClassifier.create_aug_pipeline_val(model_param_map[model_name].input_size),\n",
    "        # preprocessing_function=model_param_map[model_name].preprocessing_func,\n",
    "        workers=workers,\n",
    "        save_file_name=os.path.join(pred_result_folder, \"{}_{}.csv\".format(model_name, postfix)))\n",
    "    \n",
    "    del model\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Models' Predictions on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "model_names = ['DenseNet201', 'Xception', 'ResNeXt50']\n",
    "postfixes = ['best_balanced_acc', 'best_loss']\n",
    "\n",
    "for postfix in postfixes:\n",
    "    # Load models' predictions\n",
    "    df_dict = {model_name : pd.read_csv(os.path.join(pred_result_folder, \"{}_{}.csv\"\n",
    "                                                     .format(model_name, postfix))) for model_name in model_names}\n",
    "\n",
    "    # Check row number\n",
    "    for i in range(1, len(model_names)):\n",
    "        if len(df_dict[model_names[0]]) != len(df_dict[model_names[i]]):\n",
    "            raise ValueError(\"Row numbers are inconsistent between {} and {}\".format(model_names[0], model_names[i]))\n",
    "\n",
    "    # Check whether values of image column are consistent\n",
    "    for i in range(1, len(model_names)):\n",
    "        inconsistent_idx = np.where(df_dict[model_names[0]].image != df_dict[model_names[i]].image)[0]\n",
    "        if len(inconsistent_idx) > 0:\n",
    "            raise ValueError(\"{} values of image column are inconsistent between {} and {}\"\n",
    "                             .format(len(inconsistent_idx), model_names[0], model_names[i]))\n",
    "\n",
    "    # Copy the first model's predictions\n",
    "    df_ensemble = df_dict[model_names[0]].drop(columns=['pred_category'])\n",
    "\n",
    "    # Add up predictions\n",
    "    for category_name in category_names:\n",
    "        for i in range(1, len(model_names)):\n",
    "            df_ensemble[category_name] = df_ensemble[category_name] + df_dict[model_names[i]][category_name]\n",
    "\n",
    "    # Take average of predictions\n",
    "    for category_name in category_names:\n",
    "        df_ensemble[category_name] = df_ensemble[category_name] / len(model_names)\n",
    "\n",
    "    # Ensemble Predictions\n",
    "    df_ensemble['pred_category'] = pd.Series([np.argmax(x) for x in np.array(df_ensemble.iloc[:,1:9])])\n",
    "\n",
    "    # Save Ensemble Predictions\n",
    "    df_ensemble.to_csv(path_or_buf=os.path.join(pred_result_folder, \"Ensemble_{}.csv\".format(postfix)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Predicted Results on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score\n",
    "from visuals import plot_confusion_matrix\n",
    "from keras.utils import np_utils\n",
    "from keras_numpy_backend import categorical_crossentropy\n",
    "\n",
    "model_names = ['Vanilla', 'DenseNet201', 'Xception', 'ResNeXt50', 'Ensemble']\n",
    "postfix = 'best_balanced_acc'\n",
    "print('Model selection criteria: ', postfix)\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Load predicted results\n",
    "    file_path = os.path.join(pred_result_folder, \"{}_{}.csv\".format(model_name, postfix))\n",
    "    # file_path = os.path.join(pred_result_folder, \"{}_best_loss.csv\".format(model_name))\n",
    "    if not os.path.exists(file_path):\n",
    "        continue\n",
    "\n",
    "    print(\"========== {} ==========\".format(model_name))\n",
    "    df = pd.read_csv(file_path)\n",
    "    y_true = df['category']\n",
    "    y_pred = df['pred_category']\n",
    "\n",
    "    # Compute Balanced Accuracy\n",
    "    print('balanced_accuracy_score: ', balanced_accuracy_score(y_true, y_pred))\n",
    "    print('macro recall_score: ', recall_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "    # Compute categorical_crossentropy\n",
    "    y_true_onehot = np_utils.to_categorical(df['category'], num_classes=known_category_num)\n",
    "    y_pred_onehot = np.array(df.iloc[:,1:9])\n",
    "    print('categorical_crossentropy: ',\n",
    "          np.average(categorical_crossentropy(y_true_onehot, y_pred_onehot)))\n",
    "\n",
    "    # Compute weighted categorical_crossentropy\n",
    "    print('weighted categorical_crossentropy: ',\n",
    "          np.average(categorical_crossentropy(y_true_onehot, y_pred_onehot, class_weights=class_weights)))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    fig = plot_confusion_matrix(y_true, y_pred, category_names, normalize=True,\n",
    "                                title=\"Confusion Matrix of {}\".format(model_name),\n",
    "                                figsize=(8, 6))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from visuals import plot_grouped_2bars\n",
    "\n",
    "sample_count_val = y_true.shape[0]\n",
    "print(\"Validation set has {} samples.\\n\".format(sample_count_val))\n",
    "\n",
    "print('========== Ground Truth ==========')\n",
    "count_true = Counter(y_true)\n",
    "for i, c in enumerate(category_names):\n",
    "    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_true[i], count_true[i]*100/sample_count_val))\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Load predicted results\n",
    "    file_path = os.path.join(pred_result_folder, \"{}_{}.csv\".format(model_name, postfix))\n",
    "    if not os.path.exists(file_path):\n",
    "        continue\n",
    "\n",
    "    print(\"\\n========== {} Prediction ==========\".format(model_name))\n",
    "    df = pd.read_csv(file_path)\n",
    "    y_pred = df['pred_category']\n",
    "    \n",
    "    count_pred = Counter(y_pred)\n",
    "    for i, c in enumerate(category_names):\n",
    "        print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_pred[i], count_pred[i]*100/sample_count_val))\n",
    "\n",
    "    # Plot Prediction Distribution of Vanilla CNN\n",
    "    plot_grouped_2bars(\n",
    "        scalars=[[count_true[i] for i in range(known_category_num)],\n",
    "                 [count_pred[i] for i in range(known_category_num)]],\n",
    "        scalarlabels=['Ground Truth', 'Prediction'],\n",
    "        xticklabels=category_names,\n",
    "        title=\"Prediction Distribution of {}\".format(model_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Out-of-Distribtion CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "out_dist_image_folder = os.path.join(data_folder, 'ISIC_Archive_Out_Distribtion')\n",
    "out_dist_file = os.path.join(data_folder, 'ISIC_Archive_Out_Distribtion.csv')\n",
    "\n",
    "print(len(sorted(Path(out_dist_image_folder).glob('**/*.jpg'))), 'out-of-distribution images')\n",
    "df_out_dist = pd.DataFrame([Path(x).stem for x in sorted(Path(out_dist_image_folder).glob('**/*.jpg'))],\n",
    "                           columns =['image'] )\n",
    "df_out_dist.to_csv(out_dist_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Different Models to Predict Out-of-Distribtion Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from metrics import balanced_accuracy\n",
    "from lesion_classifier import LesionClassifier\n",
    "from vanilla_classifier import VanillaClassifier\n",
    "from base_model_param import get_transfer_model_param_map\n",
    "from keras import backend as K\n",
    "\n",
    "df_out_dist = pd.read_csv(out_dist_file)\n",
    "df_out_dist['path'] = df_out_dist.apply(lambda row : os.path.join(out_dist_image_folder, row['image']+'.jpg'), axis=1)\n",
    "\n",
    "if not os.path.exists(out_dist_pred_result_folder):\n",
    "    os.makedirs(out_dist_pred_result_folder)\n",
    "\n",
    "model_names = ['DenseNet201', 'Xception', 'ResNeXt50']\n",
    "postfixes = ['best_balanced_acc', 'best_loss', 'latest']\n",
    "model_param_map = get_transfer_model_param_map()\n",
    "\n",
    "for model_name in model_names:\n",
    "    for postfix in postfixes:\n",
    "        filename = \"{}_{}.hdf5\".format(model_name, postfix)\n",
    "        print('Load: ', filename)\n",
    "        # Load model\n",
    "        model = load_model(filepath=os.path.join(saved_model_folder, filename),\n",
    "                           custom_objects={'balanced_accuracy': balanced_accuracy(known_category_num)})\n",
    "\n",
    "        # Predict Out-of-Distribtion Set\n",
    "        df_pred = LesionClassifier.predict_dataframe(\n",
    "            model=model, df=df_out_dist,\n",
    "            category_names=category_names,\n",
    "            augmentation_pipeline=LesionClassifier.create_aug_pipeline_val(model_param_map[model_name].input_size),\n",
    "            preprocessing_function=model_param_map[model_name].preprocessing_func,\n",
    "            workers=workers,\n",
    "            save_file_name=os.path.join(out_dist_pred_result_folder, \"{}_{}.csv\".format(model_name, postfix)))\n",
    "\n",
    "        del model\n",
    "        K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Baseline and ODIN Softmax Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 main.py /home --odin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Optimal ODIN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odin import ModelAttr, compute_tpr95\n",
    "\n",
    "model_names = ['DenseNet201', 'Xception', 'ResNeXt50']\n",
    "postfixes = ['best_balanced_acc', 'best_loss', 'latest']\n",
    "\n",
    "# Baseline\n",
    "for modelattr in (ModelAttr(x, y) for x in model_names for y in postfixes):\n",
    "    print(\"===== {}_{} =====\".format(modelattr.model_name, modelattr.postfix))\n",
    "    fpr, delta = compute_tpr95(\n",
    "        in_dist_file=\"softmax_scores/Base/{}_{}_Base_In.txt\".format(modelattr.model_name, modelattr.postfix),\n",
    "        out_dist_file=\"softmax_scores/Base/{}_{}_Base_Out.txt\".format(modelattr.model_name, modelattr.postfix),\n",
    "        delta_start=0.125,\n",
    "        delta_end=1)\n",
    "    print(\"FPR:{}, Delta:{}\\n\".format(fpr, delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from odin import ModelAttr, OdinParam, compute_tpr95\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "model = 'DenseNet201_best_balanced_acc'\n",
    "temperatures = [1000, 500, 200, 100, 50, 20, 10, 5, 2, 1]\n",
    "magnitudes = np.round(np.arange(0, 0.0041, 0.0002), 4)\n",
    "\n",
    "fpr_min = sys.float_info.max\n",
    "delta_fpr_min = None\n",
    "odinparam_fpr_min = None\n",
    "for odinparam in (OdinParam(x, y) for x in temperatures for y in magnitudes):\n",
    "    print(\"===== Temperature: {}, Magnitude: {} =====\".format(odinparam.temperature, odinparam.magnitude))\n",
    "    fpr, delta = compute_tpr95(\n",
    "        in_dist_file=\"softmax_scores/{}_{}/{}_ODIN_In.txt\".format(odinparam.temperature, odinparam.magnitude, model),\n",
    "        out_dist_file=\"softmax_scores/{}_{}/{}_ODIN_Out.txt\".format(odinparam.temperature, odinparam.magnitude, model),\n",
    "        delta_start=0.125,\n",
    "        delta_end=0.135)\n",
    "    print(\"FPR:{}, Delta:{}\\n\".format(fpr, delta))\n",
    "    if fpr < fpr_min:\n",
    "        fpr_min = fpr\n",
    "        delta_fpr_min = delta\n",
    "        odinparam_fpr_min = odinparam\n",
    "print(\"FPR_Min:{}, Delta:{}, Temperature: {}, Magnitude: {}\"\n",
    "      .format(fpr_min, delta_fpr_min, odinparam_fpr_min.temperature, odinparam_fpr_min.magnitude))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
