{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Lesion Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to mount Google Drive for Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "# !ls '/content/drive/My Drive/Colab Notebooks'\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks/isic-2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp '/content/drive/My Drive/Colab Notebooks/ISIC_2019_Training_Input.zip' '/home/ISIC_2019_Training_Input.zip'\n",
    "# !cp '/content/drive/My Drive/Colab Notebooks/ISIC_2019_Training_GroundTruth.csv' '/home/ISIC_2019_Training_GroundTruth.csv'\n",
    "# !unzip -qq '/home/ISIC_2019_Training_Input.zip' -d '/home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ref https://docs.fast.ai/performance.html\n",
    "# !pip uninstall -y pillow pil jpeg libtiff libjpeg-turbo\n",
    "# !CFLAGS=\"${CFLAGS} -mavx2\" pip install --upgrade --no-cache-dir --force-reinstall --no-binary :all: --compile pillow-simd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether youâ€™re running Pillow or Pillow-SIMD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the author, if PILLOW_VERSION has a postfix, it is Pillow-SIMD0.\n",
    "# (Assuming that Pillow will never make a .postX release).\n",
    "!python -c \"from PIL import Image; print(Image.PILLOW_VERSION)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whether Pillow or Pillow-SIMD is using libjpeg-turbo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import features, Image\n",
    "from packaging import version\n",
    "\n",
    "if version.parse(Image.PILLOW_VERSION) >= version.parse(\"5.4.0\"):\n",
    "    if features.check_feature('libjpeg_turbo'):\n",
    "        print(\"libjpeg-turbo is on\")\n",
    "    else:\n",
    "        print(\"libjpeg-turbo is not on\")\n",
    "else:\n",
    "    print(\"libjpeg-turbo' status can't be derived - need Pillow(-SIMD)? >= 5.4.0 to tell, current version {}\".format(Image.PILLOW_VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm TensorFlow can see the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import platform\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "!python3 --version\n",
    "\n",
    "print('\\nTensorFlow Version: ', tf.VERSION)\n",
    "\n",
    "print('\\nNVIDIA:')\n",
    "!nvcc --version\n",
    "# !nvidia-smi\n",
    "\n",
    "print('\\nCPU:')\n",
    "!lscpu\n",
    "\n",
    "print('\\nMemory:')\n",
    "!cat /proc/meminfo\n",
    "\n",
    "print('\\nOS:')\n",
    "print(platform.platform())\n",
    "\n",
    "print('\\nDevices:')\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# dermoscopic images folder path\n",
    "data_folder = 'C:\\ISIC_2019'\n",
    "# data_folder = '/home'\n",
    "# data_folder = '/home/jupyter'\n",
    "derm_image_folder = os.path.join(data_folder, 'ISIC_2019_Training_Input')\n",
    "df_ground_truth = pd.read_csv(os.path.join(data_folder, 'ISIC_2019_Training_GroundTruth.csv'))\n",
    "\n",
    "# Category names not include UNK\n",
    "category_names = list(df_ground_truth.columns.values[1:9])\n",
    "known_category_num = len(category_names)\n",
    "print(\"Number of known categories: {}\".format(known_category_num))\n",
    "print(category_names, '\\n')\n",
    "\n",
    "# mapping from category to index\n",
    "print('Category to Index:')\n",
    "category_to_index = dict((c, i) for i, c in enumerate(category_names))\n",
    "print(category_to_index, '\\n')\n",
    "\n",
    "df_ground_truth['path'] = df_ground_truth.apply(lambda row : os.path.join(derm_image_folder, row['image']+'.jpg'), axis=1)\n",
    "df_ground_truth['category'] = pd.Series([np.argmax(x) for x in np.array(df_ground_truth.iloc[:,1:9])], name='category')\n",
    "count_per_category = Counter(df_ground_truth['category'])\n",
    "total_sample_count = sum(count_per_category.values())\n",
    "print(\"Original training data has {} samples.\".format(total_sample_count))\n",
    "\n",
    "for i, c in enumerate(category_names):\n",
    "    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_per_category[i], count_per_category[i]*100/total_sample_count))\n",
    "\n",
    "fig = plt.bar(count_per_category.keys(), count_per_category.values())\n",
    "\n",
    "df_ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and Split Original Training Data into Training  and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(df_ground_truth, stratify=df_ground_truth['category'], test_size=0.2, random_state=1)\n",
    "\n",
    "sample_count_train = df_train.shape[0]\n",
    "print(\"Training set has {} samples.\".format(sample_count_train))\n",
    "count_per_category_train = Counter(df_train['category'])\n",
    "for i, c in enumerate(category_names):\n",
    "    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_per_category_train[i], count_per_category_train[i]*100/sample_count_train))\n",
    "    \n",
    "sample_count_val = df_val.shape[0]\n",
    "print(\"\\nValidation set has {} samples.\".format(sample_count_val))\n",
    "count_per_category_val = Counter(df_val['category'])\n",
    "for i, c in enumerate(category_names):\n",
    "    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_per_category_val[i], count_per_category_val[i]*100/sample_count_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weights based on the Traning Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Compute class weights for imbalanced data\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(df_train['category']), df_train['category'])\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "# class_weight_dict = dict(zip(category_names, class_weights))\n",
    "print('Class Weights:')\n",
    "print(class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples of each Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "category_groups = df_train.groupby('category')\n",
    "\n",
    "# Number of samples for each category\n",
    "num_per_category = 3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=known_category_num, ncols=num_per_category, figsize=(9, 24))\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "for idx, val in enumerate(category_names):\n",
    "    i = 0\n",
    "    for index, row in category_groups.get_group(idx).head(num_per_category).iterrows():\n",
    "        ax = axes[idx, i]\n",
    "        ax.imshow(plt.imread(row['path']))\n",
    "        ax.set_xlabel(row['image'])\n",
    "        if ax.is_first_col():\n",
    "            ax.set_ylabel(val, fontsize=20)\n",
    "            ax.yaxis.label.set_color('blue')\n",
    "        i += 1\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path, size=(224, 224)):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=size)\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths, size=(224, 224)):\n",
    "    list_of_tensors = [path_to_tensor(img_path, size) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://stackoverflow.com/a/45947435/2437361\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the mean of the per-class accuracies.\n",
    "    Same as sklearn.metrics.balanced_accuracy_score and sklearn.metrics.recall_score with macro average\n",
    "    \"\"\"\n",
    "    y_true_argmax = K.argmax(y_true, axis=1)\n",
    "    y_pred_argmax = K.argmax(y_pred, axis=1)\n",
    "    mean_accuracy, update_op = tf.metrics.mean_per_class_accuracy(y_true_argmax, y_pred_argmax, known_category_num)\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    with tf.control_dependencies([update_op]):\n",
    "       mean_accuracy = tf.identity(mean_accuracy)\n",
    "    \n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vanilla CNN as benchmark model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_iterator import ImageIterator\n",
    "from Augmentor import Pipeline\n",
    "from Augmentor.Operations import CropPercentageRange\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "\n",
    "input_size = (224, 224)\n",
    "batch_size = 40\n",
    "data_format = K.image_data_format()\n",
    "\n",
    "### Training Data Generator\n",
    "#TODO Maybe remove black borders\n",
    "vanilla_p_train = Pipeline()\n",
    "# Random crop\n",
    "vanilla_p_train.add_operation(CropPercentageRange(probability=1, min_percentage_area=0.8, max_percentage_area=1, centre=False))\n",
    "# Rotate an image by either 90, 180, or 270 degrees randomly\n",
    "vanilla_p_train.rotate_random_90(probability=0.5)\n",
    "# Flip the image along its vertical axis\n",
    "vanilla_p_train.flip_top_bottom(probability=0.5)\n",
    "# Flip the image along its horizontal axis\n",
    "vanilla_p_train.flip_left_right(probability=0.5)\n",
    "# Random change brightness of an image\n",
    "vanilla_p_train.random_brightness(probability=0.5, min_factor=0.9, max_factor=1.1)\n",
    "# Random change saturation of an image\n",
    "vanilla_p_train.random_color(probability=0.5, min_factor=0.9, max_factor=1.1)\n",
    "# Resize an image\n",
    "vanilla_p_train.resize(probability=1, width=input_size[0], height=input_size[1])\n",
    "vanilla_p_train.status()\n",
    "\n",
    "generator_train = ImageIterator(\n",
    "    image_paths=df_train['path'].tolist(),\n",
    "    labels=np_utils.to_categorical(df_train['category'], num_classes=known_category_num),\n",
    "    augmentation_pipeline=vanilla_p_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    rescale=1./255,\n",
    "    pregen_augmented_images=False,\n",
    "    data_format=data_format\n",
    ")\n",
    "\n",
    "\n",
    "### Validation Data Generator\n",
    "vanilla_p_val = Pipeline()\n",
    "# Resize an image\n",
    "vanilla_p_val.resize(probability=1, width=input_size[0], height=input_size[1])\n",
    "vanilla_p_val.status()\n",
    "\n",
    "generator_val = ImageIterator(\n",
    "    image_paths=df_val['path'].tolist(),\n",
    "    labels=np_utils.to_categorical(df_val['category'], num_classes=known_category_num),\n",
    "    augmentation_pipeline=vanilla_p_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    rescale=1./255,\n",
    "    pregen_augmented_images=True, #Since the augmentation pipeline only contains a resize operation.\n",
    "    data_format=data_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print some info for debugging\n",
    "# images, labels = next(generator_train)\n",
    "# # print(len(vanilla_p_train.augmentor_images))\n",
    "# print(images.shape)\n",
    "# print(labels.shape)\n",
    "# plt.imshow(images[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "lr_start = 1e-3 # Starting learning rate\n",
    "\n",
    "# Define vanilla CNN\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(input_size[0], input_size[1], 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(known_category_num, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=lr_start), loss='categorical_crossentropy', metrics=[balanced_accuracy, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dispaly images of a batch for debugging\n",
    "# images, labels = next(generator_train)\n",
    "\n",
    "# fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "# for i, img in enumerate(images):\n",
    "#     ax = fig.add_subplot(4, 10, i + 1, xticks=[], yticks=[])\n",
    "#     ax.imshow(np.uint8(255 * img))\n",
    "#     image_idx = np.argmax(labels[i])\n",
    "#     ax.set(title=category_names[image_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the vanilla CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "import os\n",
    "\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "\n",
    "checkpoint_balanced_acc = ModelCheckpoint(\n",
    "    filepath='saved_models/vanilla_best_balanced_acc.hdf5',\n",
    "    monitor='val_balanced_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "checkpoint_loss = ModelCheckpoint(\n",
    "    filepath='saved_models/vanilla_best_loss.hdf5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "# Reduce learning rate when the validation loss has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Stop training when the validation loss has stopped improving.\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=22, verbose=1)\n",
    "\n",
    "# Callback that streams epoch results to a csv file.\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "csv_logger = CSVLogger('logs/vanilla.training.csv', append=False)\n",
    "\n",
    "epoch_num = 125\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generator_train,\n",
    "    class_weight=class_weight_dict,\n",
    "    max_queue_size=10,\n",
    "    workers=os.cpu_count(),\n",
    "    use_multiprocessing=False,\n",
    "    steps_per_epoch=sample_count_train//batch_size,\n",
    "    epochs=epoch_num,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint_balanced_acc, checkpoint_loss,\n",
    "               reduce_lr, early_stop, csv_logger],\n",
    "    validation_data=generator_val,\n",
    "    validation_steps=sample_count_val//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visuals import *\n",
    "\n",
    "plot_complexity_graph('logs/vanilla.training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "vanilla_model = load_model(\n",
    "    filepath='saved_models/vanilla_best_balanced_acc.hdf5',\n",
    "    custom_objects={'balanced_accuracy': balanced_accuracy})\n",
    "# vanilla_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Balanced Accuracy on all Validation Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score\n",
    "\n",
    "generator = ImageIterator(\n",
    "    image_paths=df_val['path'].tolist(),\n",
    "    labels=np_utils.to_categorical(df_val['category'], num_classes=known_category_num),\n",
    "    augmentation_pipeline=vanilla_p_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, #shuffle must be False otherwise will get a wrong balanced accuracy\n",
    "    rescale=1./255,\n",
    "    pregen_augmented_images=False, # Only 1 epoch.\n",
    "    data_format=data_format\n",
    ")\n",
    "\n",
    "# print(len(generator))\n",
    "predicted_vector = vanilla_model.predict_generator(generator, verbose=0, workers=os.cpu_count())\n",
    "\n",
    "y_true = df_val['category'].values\n",
    "y_pred = np.argmax(predicted_vector, axis=1)\n",
    "\n",
    "print('balanced_accuracy_score: ', balanced_accuracy_score(y_true, y_pred))\n",
    "# print('macro recall_score: ', recall_score(y_true, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Dermoscopic Images with the Vanilla CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def vanilla_classify(img_path, topk=5):\n",
    "    predicted_vector = vanilla_model.predict(path_to_tensor(img_path))\n",
    "    idx_topk = np.argsort(-predicted_vector)[0, :topk]\n",
    "    probs = np.take(predicted_vector, idx_topk)\n",
    "    names = [category_names[idx] for idx in idx_topk]\n",
    "    \n",
    "    return idx_topk, names, probs\n",
    "\n",
    "topk = 8\n",
    "df_row = df_val.iloc[random.randrange(len(df_val.index))]\n",
    "idx_topk, names, probs = vanilla_classify(df_row['path'], topk=topk)\n",
    "# print(probs)\n",
    "\n",
    "# Set up plot\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# Set up title\n",
    "fig.suptitle(df_row['image'])\n",
    "\n",
    "# Input Image\n",
    "ax1.set_title(category_names[df_row['category']])\n",
    "ax1.imshow(plt.imread(df_row['path']))\n",
    "\n",
    "# Plot probabilities bar chart\n",
    "ax2.set_title(\"Top {0} probabilities\".format(topk))\n",
    "ax2.barh(np.arange(topk), probs)\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(topk))\n",
    "ax2.set_yticklabels(names, size='medium')\n",
    "ax2.yaxis.tick_right()\n",
    "ax2.set_xlim(0, 1.0)\n",
    "ax2.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(preprocess_input_densenet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet201, preprocess_input as preprocess_input_densenet\n",
    "from keras.applications.xception  import Xception, preprocess_input as preprocess_input_xception\n",
    "from keras.applications.nasnet import NASNetLarge, preprocess_input as preprocess_input_nasnet\n",
    "from lesion_classifier import LesionClassifier\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from base_model_param import BaseModelParam\n",
    "import os\n",
    "\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "    \n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "    \n",
    "lr_start = 1e-3 # starting learning rate.\n",
    "epoch_num = 125\n",
    "workers = os.cpu_count()\n",
    "\n",
    "# Reduce learning rate when the validation loss has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Stop training when the validation loss has stopped improving.\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=22, verbose=1)\n",
    "\n",
    "base_model_params = [\n",
    "    BaseModelParam(input_size=(224, 224),\n",
    "                   base_model=DenseNet201(include_top=False, weights='imagenet'),\n",
    "                   base_model_name='DenseNet201',\n",
    "                   layers_trainable=True,\n",
    "                   preprocessing_func=preprocess_input_densenet),\n",
    "    BaseModelParam(input_size=(299, 299),\n",
    "                   base_model=Xception(include_top=False, weights='imagenet'),\n",
    "                   base_model_name='Xception',\n",
    "                   layers_trainable=True,\n",
    "                   preprocessing_func=preprocess_input_xception),\n",
    "    BaseModelParam(input_size=(331, 331),\n",
    "                   base_model=NASNetLarge(include_top=False, weights='imagenet'),\n",
    "                   base_model_name='NASNetLarge',\n",
    "                   layers_trainable=False,\n",
    "                   preprocessing_func=preprocess_input_nasnet)\n",
    "]\n",
    "\n",
    "for model_param in base_model_params:\n",
    "    classifier = LesionClassifier(\n",
    "        preprocessing_func=model_param.preprocessing_func,\n",
    "        image_data_format=K.image_data_format(),\n",
    "        input_size=model_param.input_size,\n",
    "        image_paths_train=df_train['path'].tolist(),\n",
    "        categories_train=np_utils.to_categorical(df_train['category'], num_classes=known_category_num),\n",
    "        image_paths_val=df_val['path'].tolist(),\n",
    "        categories_val=np_utils.to_categorical(df_val['category'], num_classes=known_category_num)\n",
    "    )\n",
    "\n",
    "    model = classifier.create_model(\n",
    "        base_model=model_param.base_model,\n",
    "        fc_layers=[512],\n",
    "        num_classes=known_category_num,\n",
    "        dropout=0.3,\n",
    "        base_model_layers_trainable=model_param.layers_trainable\n",
    "    )\n",
    "    model.summary()\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(lr=lr_start), loss='categorical_crossentropy', metrics=[balanced_accuracy, 'accuracy'])\n",
    "    \n",
    "    checkpoint_balanced_acc = ModelCheckpoint(\n",
    "        filepath='saved_models/{}_best_balanced_acc.hdf5'.format(model_param.base_model_name),\n",
    "        monitor='val_balanced_accuracy',\n",
    "        verbose=1,\n",
    "        save_best_only=True)\n",
    "\n",
    "    checkpoint_loss = ModelCheckpoint(\n",
    "        filepath='saved_models/{}_best_loss.hdf5'.format(model_param.base_model_name),\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True)\n",
    "    \n",
    "    # Callback that streams epoch results to a csv file.\n",
    "    csv_logger = CSVLogger('logs/{}.training.csv'.format(model_param.base_model_name), append=False)\n",
    "    \n",
    "    model.fit_generator(\n",
    "        classifier.generator_train,\n",
    "        class_weight=class_weight_dict,\n",
    "        max_queue_size=10,\n",
    "        workers=workers,\n",
    "        use_multiprocessing=False,\n",
    "        steps_per_epoch=sample_count_train//classifier.batch_size,\n",
    "        epochs=epoch_num,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint_balanced_acc, checkpoint_loss,\n",
    "                   reduce_lr, early_stop, csv_logger],\n",
    "        validation_data=classifier.generator_val,\n",
    "        validation_steps=sample_count_val//classifier.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visuals import *\n",
    "\n",
    "for model_param in base_model_params:\n",
    "    plot_complexity_graph('logs/{}.training.csv'.format(model_param.base_model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
