{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "from metrics import balanced_accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras_numpy_backend import categorical_crossentropy, softmax\n",
    "from image_iterator import ImageIterator\n",
    "from lesion_classifier import LesionClassifier\n",
    "from utils import preprocess_input as preprocess_input_trainset\n",
    "\n",
    "temperature = 1000\n",
    "noise_magnitude = 0.0014 # perturbation magnitude\n",
    "num_classes = 8\n",
    "image_data_format = K.image_data_format()\n",
    "\n",
    "model_filepath = 'saved_models/DenseNet201_best_balanced_acc.hdf5'\n",
    "model = load_model(filepath=model_filepath, custom_objects={'balanced_accuracy': balanced_accuracy(num_classes)})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from image_iterator import ImageIterator\n",
    "from lesion_classifier import LesionClassifier\n",
    "from utils import preprocess_input as preprocess_input_trainset\n",
    "\n",
    "# Load predicted results of validation set\n",
    "df = pd.read_csv('predict_results/DenseNet201_best_balanced_acc.csv')\n",
    "data_folder = 'C:/ISIC_2019'\n",
    "derm_image_folder = os.path.join(data_folder, 'ISIC_2019_Training_Input')\n",
    "df['path'] = df.apply(lambda row : os.path.join(derm_image_folder, row['image']+'.jpg'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_score_folder = 'softmax_scores'\n",
    "if not os.path.exists(softmax_score_folder):\n",
    "    os.makedirs(softmax_score_folder)\n",
    "    \n",
    "f1 = open(os.path.join(softmax_score_folder, 'confidence_Base_In.txt'), 'w')\n",
    "f2 = open(os.path.join(softmax_score_folder, 'confidence_Base_Out.txt'), 'w')\n",
    "g1 = open(os.path.join(softmax_score_folder, 'confidence_Our_In.txt'), 'w')\n",
    "g2 = open(os.path.join(softmax_score_folder, 'confidence_Our_Out.txt'), 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confidence of the output, no perturbation added here, no temperature scaling used\n",
    "for index, row in df.iterrows():\n",
    "    softmax_probs = row[1:9]\n",
    "    softmax_score = np.max(softmax_probs)\n",
    "    f1.write(\"{}, {}, {}\\n\".format(temperature, noise_magnitude, softmax_score))\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference https://stackoverflow.com/a/53650141/2437361\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "# layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# an input layer to feed labels\n",
    "y_true = keras.engine.input_layer.Input(shape=(num_classes,))\n",
    "\n",
    "### Compute loss based on model's output, temperature Scaling, and true labels\n",
    "# ce = K.mean(K.categorical_crossentropy(y_true, model.output))\n",
    "scaled_dense_pred_output = model.get_layer('dense_pred').output / temperature\n",
    "loss = K.categorical_crossentropy(y_true, K.softmax(scaled_dense_pred_output))\n",
    "\n",
    "# Compute gradient of loss with respect to inputs\n",
    "grad_loss = K.gradients(loss, model.inputs)\n",
    "\n",
    "# Create a function to be able to run this computation graph\n",
    "# The learning phase flag is a bool tensor (0 = test, 1 = train)\n",
    "compute_perturbations = K.function(model.inputs + [y_true] + [K.learning_phase()],\n",
    "                                   grad_loss)\n",
    "\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer\n",
    "# get_dense_pred_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n",
    "#                                          [model.get_layer('dense_pred').output])\n",
    "get_dense_pred_layer_output = K.function(model.inputs + [K.learning_phase()],\n",
    "                                         [model.get_layer('dense_pred').output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = test, 1 = train\n",
    "learning_phase = 0\n",
    "\n",
    "generator = ImageIterator(\n",
    "    image_paths=df['path'].tolist(),\n",
    "    labels=np_utils.to_categorical(df['category'], num_classes=num_classes),\n",
    "    augmentation_pipeline=LesionClassifier.create_aug_pipeline_val((224, 224)),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    rescale=None,\n",
    "    preprocessing_function=preprocess_input_trainset,\n",
    "    pregen_augmented_images=False,\n",
    "    data_format=image_data_format\n",
    ")\n",
    "\n",
    "for i, (image, y_true_onehot) in enumerate(generator):\n",
    "    print('y_true_onehot: ', y_true_onehot)\n",
    "    \n",
    "    perturbations = compute_perturbations([image, y_true_onehot, learning_phase])[0]\n",
    "    print('perturbations: ', perturbations.shape)\n",
    "    print(perturbations[0][0])\n",
    "    \n",
    "    # Get sign of perturbations\n",
    "    perturbations = np.sign(perturbations)\n",
    "    print(perturbations)\n",
    "    \n",
    "    # Normalize the perturbations to the same space of image\n",
    "    # https://github.com/facebookresearch/odin/issues/5\n",
    "    # TODO: I should use ISIC Training Set std = [0.2422, 0.2235, 0.2315]\n",
    "    perturbations[0][0] = (perturbations[0][0])/(63.0/255.0)\n",
    "    perturbations[0][1] = (perturbations[0][1])/(62.1/255.0)\n",
    "    perturbations[0][2] = (perturbations[0][2])/(66.7/255.0)\n",
    "    \n",
    "    # Add perturbations to images\n",
    "    perturbative_image = image - noise_magnitude * perturbations\n",
    "    \n",
    "    # Calculate the confidence after adding perturbations\n",
    "    dense_pred_output = get_dense_pred_layer_output([perturbative_image, learning_phase])[0]\n",
    "    dense_pred_output = dense_pred_output / temperature\n",
    "    softmax_probs = softmax(dense_pred_output)\n",
    "    softmax_score = np.max(softmax_probs)\n",
    "    g1.write(\"{}, {}, {}\\n\".format(temperature, noise_magnitude, softmax_score))\n",
    "    \n",
    "    break\n",
    "    \n",
    "g1.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
